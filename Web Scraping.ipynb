{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055fd47b-3f06-44d2-89b0-3b3294b9e675",
   "metadata": {},
   "source": [
    "## Web Scraping\n",
    "\n",
    "In this project I am to test my web scraping and machine learning knowledge.\n",
    "\n",
    "\n",
    "#### The assignment\n",
    "\n",
    "Create a new model that is able to extract products from Furniture Stores. \n",
    "\n",
    "#### Inputs\n",
    "\n",
    "You’ll be given a list of URLs (700 URLs) from furnitures stores sites. Most will have products on them, some won’t, some won’t even work at all.\n",
    "\n",
    "#### Outputs\n",
    "\n",
    "We expect a list of product names extracted from every URL, but you can get creative in presenting your results. See what the most popular product is, aggregate all the pages of a site etc.\n",
    "Try to showcase what your solution is best at.\n",
    "\n",
    "#### Guidelines\n",
    "\n",
    "An approach that usually works well with such extraction problems is to create a NER (Named Entity Recognition) model and train it to find your entities (you have one entity, ‘PRODUCT’).\n",
    "\n",
    "- In order to create such a model you need training data, you can also extract that from the input pages.\n",
    "- Crawl ~100 pages from the list above & extract the text from it.\n",
    "- Find a way to tag some sample products from these texts.\n",
    "- Train a new model from the examples you just made.\n",
    "- Use it to extract product names from some new, unseen pages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0dfef1a-4ebe-439b-8137-bf1329df81ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We start by importing the necessary packages\n",
    "\n",
    "import csv\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "#Some of the 700 links present in our \"furniture.csv\" don't work, or don't have any products at all.\n",
    "#We create an exception for these so they get ignored.\n",
    "\n",
    "with open('furniture.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "        try:\n",
    "            page = requests.get(row[0])\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            print(text)\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            continue\n",
    "\n",
    "#2) Find a way to tag some sample products from these texts.\n",
    "\n",
    "import csv\n",
    "from nltk import word_tokenize, pos_tag\n",
    "\n",
    "with open('furniture.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            page = requests.get(row[0])\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            tokens = word_tokenize(text)                               #tokenizing the text\n",
    "            tagged_words = pos_tag(tokens)                              #tagging the tokens\n",
    "            print(tagged_words)\n",
    "\n",
    "#3) Train a new model from the examples you just made.\n",
    "\n",
    "import csv\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "training_data = []                                                    #creating an empty list for training data\n",
    "\n",
    "with open('furniture.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            page = requests.get(row[0])\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            tokens = word_tokenize(text)\n",
    "            tagged_words = pos_tag(tokens)\n",
    "            named_entities = ne_chunk(tagged_words)                    #chunking the tagged words\n",
    "            training_data.append(named_entities)                       #appending the chunked words to the training data list\n",
    "\n",
    "#4)Use it to extract product names from some new, unseen pages.\n",
    "\n",
    "import csv\n",
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from nltk.tag import UnigramTagger, BigramTagger\n",
    "\n",
    "# training the tagger\n",
    "train_data = training_data[:500] # using first 500 entries of training data to train the tagger\n",
    "unigram_tagger = UnigramTagger(train_data)\n",
    "bigram_tagger = BigramTagger(train_data, backoff=unigram_tagger)\n",
    "\n",
    "# testing the tagger\n",
    "with open('furniture.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file, delimiter=',')\n",
    "    line_count = 0\n",
    "    product_names = []\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            page = requests.get(row[0])\n",
    "            soup = BeautifulSoup(page.text, 'html.parser')\n",
    "            text = soup.get_text()\n",
    "            tokens = word_tokenize(text)\n",
    "            tagged_words = bigram_tagger.tag(tokens) # tagging the tokens with the trained tagger\n",
    "            named_entities = ne_chunk(tagged_words) # chunking the tagged words\n",
    "            iob_tagged = tree2conlltags(named_entities) # converting the chunked words to IOtags\n",
    "            product_list = [word for word, pos, chunk in iob_tagged if chunk == \"B-PRODUCT\"] # extracting the product names \n",
    "            product_names.extend(product_list)\n",
    "\n",
    "# writing the extracted product names to a CSV file\n",
    "with open('extracted_products.csv', mode='w', newline='') as product_file:\n",
    "    product_writer = csv.writer(product_file)\n",
    "    product_writer.writerow(['Product Name'])\n",
    "    for product in product_names:\n",
    "        product_writer.writerow([product])\n",
    "\n",
    "# reimporting the CSV file and finding the most frequent extracted word\n",
    "from collections import Counter\n",
    "\n",
    "with open('extracted_products.csv') as csv_file:\n",
    "    csv_reader = csv.reader(csv_file)\n",
    "    line_count = 0\n",
    "    product_names = []\n",
    "    for row in csv_reader:\n",
    "        if line_count == 0:\n",
    "            line_count += 1\n",
    "            continue\n",
    "        else:\n",
    "            line_count += 1\n",
    "            product_names.append(row[0])\n",
    "\n",
    "# finding the most frequent extracted word\n",
    "most_frequent_word = Counter(product_names).most_common(1)[0][0]\n",
    "print(f\"The most frequent extracted word is: {most_frequent_word}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
