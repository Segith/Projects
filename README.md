# Portfolio

## Machine Learning

### 1) Project Titanic
In the next project we plan to use the famous Titanic.csv dataset to 
1) Do a rigorous data analysis
2) To predict based on variables the chance of survival

[Project Titanic](https://github.com/Segith/Projects/blob/main/Proiect%20TITANIC.ipynb)


### 2) Apriori Alghoritm

In this project we applied Apriori Alghoritm to the Bread Basket database to answer the following questions:
1) Which are the most / least purchased products

2) What is the time interval when the shop was having the most purchases

3) What is the "busiest" hour

4) What time of the day records the highest number of purchases

5) What are the association rules between the products

[Apriori Alghoritm](https://github.com/Segith/Projects/blob/main/Apropri.ipynb)


### 3) Costumer Segmentation

In this project we used K-means and Principal Component Analysis to create costumer groups.

[Costumer Segmentation](https://github.com/Segith/Projects/blob/main/Costumer%20Segmentation.ipynb)

### 4) Fake News Classifier 

Fake news is a big issue of the 21st century. In ancient Athens, fake news spread through the speeches of politicians or sophists. Aristotle solves this problem by laying the foundations of logic. I think it is our duty to do the same, but using more advanced tools. In this project created classification models with the aim to predict and classify fake news.

[Fake News Classifier](https://github.com/Segith/Projects/blob/main/Fake%20News%20Project.ipynb)

### 5) Beer Production in Australia forecast for 1996

In this project we used 3 models: Linear Regression, SARIMA and Exponential Smoothing in order to predict the montly beer production in Australia for the year 1996. We used a database with the montly beer production in Australia from 1950 to 1995. 
[Beer Production Forecast](https://github.com/Segith/Projects/blob/main/Monthly%20Beer%20Production%20Australia%20for%201996.ipynb)


### 6) Imputation with Scikit-Learn
When NaN's are present in a database, most people chose the easiest way possible; drop them. This could end up in biasing the analysis of data and the loss of valuable data. However techniques to solve this are available, such as imputing data with averages or medians etc. More advanced techniques include imputation based on the values of other columns, a thing we are going to explore today.

[Imputation using scikit-learn](https://github.com/Segith/Projects/blob/main/Imputation.ipynb)

### 7) EDA with PySpark
In this project I am exploring two databases and I am applying both supervised and unsupervised machine learning alghoritms using the Pyspark infrastructure.

[PySpark](https://github.com/Segith/Projects/blob/main/EDA%20with%20PySpark.ipynb)

### 8) MongoDB (PyMongo)
In this project I am exploring the different functionalities of Mongo by also creating a database

[MongoDB](https://github.com/Segith/Projects/blob/main/MongoDB.ipynb)

### 9) BigQuery
In this project I am using 2 databases (Stack Overflow and Hacker News). The two databases contain millions of rows and they are a perfect fit for exploration using BigQuery

[BigQuery](https://github.com/Segith/Projects/blob/main/BigQuery.ipynb)

## Web Scraping
In this project I use a database of about 700 URLs. These URLs contain various stores webpages. I am using a crawler for about 100 of these pages
to scrape them and create a model that automatically scrapes the rest of the pages of "PRODUCTS".

[Web Scraping](https://github.com/Segith/Projects/blob/main/Web%20Scraping.ipynb)

## Deep Learning
[Age and Gender Classification using Keras](https://github.com/Segith/Projects/blob/main/Age%20and%20Gender%20classification.zip)


## SQL

[SQL and Data Analysis](https://github.com/Segith/Projects/blob/main/SQL%2C%20EDA%20and%20DA.ipynb)



