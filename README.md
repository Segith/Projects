# Portfolio

## Machine Learning

## 1) Project Titanic
In the next project we plan to use the famous Titanic.csv dataset to 
1) Do a rigorous data analysis
2) To predict based on variables the chance of survival

[Project Titanic](https://github.com/Segith/Projects/blob/main/Proiect%20TITANIC.ipynb)


## 2) Apriori Alghoritm

In this project we applied Apriori Alghoritm to the Bread Basket database to answer the following questions:
1) Which are the most / least purchased products

2) What is the time interval when the shop was having the most purchases

3) What is the "busiest" hour

4) What time of the day records the highest number of purchases

5) What are the association rules between the products

[Apriori Alghoritm](https://github.com/Segith/Projects/blob/main/Apropri.ipynb)


## 3) Costumer Segmentation

In this project we used K-means and Principal Component Analysis to create costumer groups.

[Costumer Segmentation](https://github.com/Segith/Projects/blob/main/Costumer%20Segmentation.ipynb)

## 4) Fake News Classifier 

Fake news is a big issue of the 21st century. In ancient Athens, fake news spread through the speeches of politicians or sophists. Aristotle solves this problem by laying the foundations of logic. I think it is our duty to do the same, but using more advanced tools. In this project created classification models with the aim to predict and classify fake news.

[Fake News Classifier](https://github.com/Segith/Projects/blob/main/Fake%20News%20Project.ipynb)

## 5) Beer Production in Australia forecast for 1996

In this project we used 3 models: Linear Regression, SARIMA and Exponential Smoothing in order to predict the montly beer production in Australia for the year 1996. We used a database with the montly beer production in Australia from 1950 to 1995. 
[Beer Production Forecast](https://github.com/Segith/Projects/blob/main/Monthly%20Beer%20Production%20Australia%20for%201996.ipynb)


## 6) Imputation with Scikit-Learn
When NaN's are present in a database, most people chose the easiest way possible; drop them. This could end up in biasing the analysis of data and the loss of valuable data. However techniques to solve this are available, such as imputing data with averages or medians etc. More advanced techniques include imputation based on the values of other columns, a thing we are going to explore today.

[Imputation using scikit-learn](https://github.com/Segith/Projects/blob/main/Imputation.ipynb)

# Deep Learning
[Age and Gender Classification using Keras](https://github.com/Segith/Projects/blob/main/Age%20and%20Gender%20classification.zip)

# SQL

[SQL and Data Analysis](https://github.com/Segith/Projects/blob/main/SQL%2C%20EDA%20and%20DA.ipynb)

Imputation with Scikit-Learnhttps://github.com/Segith/Projects/blob/main/Imputation.ipynb
